import time
from typing import Optional

import torch
import torch.nn as nn
from torch import Tensor
from transformers import PreTrainedTokenizerBase

from lego_gpt.inference.base_sampler import SamplingConfig
from lego_gpt.inference.greedy_sampler import GreedySampler


B_INST, E_INST = "[INST]", "[/INST]"

class SpeculativeDecoding(GreedySampler):

    def __init__(
        self, 
        model: nn.Module, 
        draft_model: nn.Module, 
        tokenizer: PreTrainedTokenizerBase, 
        compile: bool = False,
        is_chat_model: bool = False,
        verbose: bool = False
    ) -> None:
        
        self.model = model
        self.draft_model = draft_model
        self.is_chat_model = is_chat_model
        self.verbose = verbose
        
        if compile:
            self._compile()
        
        super().__init__(model=self.model, tokenizer=tokenizer, verbose=verbose) # Target sampler
        self._draft_sampler = GreedySampler(model=self.draft_model, tokenizer=tokenizer, verbose=verbose)

        self._device_sync()

    def generate(
        self, 
        prompt: str, 
        max_new_tokens: int = 200, 
        speculate_k: int = 8,
        sampling_config: Optional[SamplingConfig] = None,
    ) -> str:
        
        if self.is_chat_model:
            prompt = f"{B_INST} {prompt.strip()} {E_INST}"

        encoded = self.encode(prompt)["input_ids"]
        prompt_length = encoded.shape[-1]
        total_length = prompt_length + max_new_tokens

        start_time = time.perf_counter()
        out = torch.empty(total_length, device=self.device, dtype=torch.long)
        out[:prompt_length] = encoded.squeeze()

        # Generate the first token using the target model
        # Both target model and draft model uses dynamic KV cache
        # We need to make sure both caches are aligned
        next_token_id, _ = self.generate_one_token(input_ids=encoded, sampling_config=sampling_config, use_cache=True)
        out[prompt_length] = next_token_id
        self._draft_sampler.forward(input_ids=encoded, use_cache=True)

        # Since the prompt tokens (0 to prompt_length-1) are already added to the KV cache,
        # we start the speculative decoding from the `prompt_length`
        # `position_ids` will be created on the fly with the past_key_values length

        # Keep track of the no. of times n tokens generated by the draft model were accepted
        accept_counts = [0] * (speculate_k + 1) 
        generated_tokens = prompt_length
        while generated_tokens < total_length - 1:
            curr_token_id = next_token_id.view(1, -1)
            next_token_ids = self._speculative_decode(curr_token_id, speculate_k, sampling_config)
            accept_counts[len(next_token_ids) - 1] += 1
            num_added = min(total_length - generated_tokens - 1, len(next_token_ids)) # Limit tokens to `max_new_tokens`
            out[generated_tokens + 1: generated_tokens + num_added + 1] = next_token_ids[:num_added]
            generated_tokens += num_added
            next_token_id = next_token_ids[-1]
        
        time_taken = time.perf_counter() - start_time
        if self.verbose:
            tokens_per_sec = max_new_tokens / time_taken
            print(
                f"Time for inference: {time_taken:.02f} sec total, {tokens_per_sec:.02f} tokens/sec",
                f"Bandwidth achieved: {self.model_size * tokens_per_sec / 1e9:.02f} GB/s",
                f"Acceptance: {accept_counts}",
                sep="\n"
            )

        output = self.decode(token_ids=out)
        SpeculativeDecoding.clear_memory()
        return output

    def _speculative_decode(
        self, 
        curr_token_id: Tensor,
        speculate_k: int,
        sampling_config: Optional[SamplingConfig] = None
    ) -> Tensor:
        
        # [Sequentially] Generate `speculate_k` tokens using the draft model
        draft_token_ids, draft_probs = self._draft_sampler.generate_n_tokens(
            input_ids=curr_token_id, 
            num_new_tokens=speculate_k, 
            sampling_config=sampling_config, 
            use_cache=True
        )
        draft_token_ids = torch.cat(draft_token_ids)
        draft_probs = torch.stack(draft_probs)

        # [Single pass] Combine the `curr_token_id` and `draft_token_ids`, and pass it as input to
        # the target model to get the token probabilities
        logits = self.forward(input_ids=torch.cat((curr_token_id, draft_token_ids.view(1, -1)), dim=1), use_cache=True)
        target_probs = SpeculativeDecoding.logits_to_probs(logits[0], sampling_config)
        
        # Apply heuristics to select token_ids. At least one token is selected.
        next_token_ids = self._speculative_sampling(
            target_probs=target_probs, 
            draft_probs=draft_probs,
            draft_token_ids=draft_token_ids
        )
        return next_token_ids

    def _speculative_sampling(
        self,
        target_probs: Tensor, # Shape: (1 + speculate_k, vocab_size)
        draft_probs: Tensor, # Shape: (speculate_k, vocab_size)
        draft_token_ids: Tensor, # Shape: (speculate_k,)
    ) -> Tensor:
        """Speculative sampling method as described in \
        [Accelerating Large Language Model Decoding with \
        Speculative Sampling](https://arxiv.org/pdf/2302.01318.pdf).
        """

        speculate_k, _ = draft_probs.shape
        prob_indicies = torch.arange(speculate_k, device=target_probs.device)
        selected_draft_probs = draft_probs[prob_indicies, draft_token_ids] # Shape: (speculate_k,)
        selected_target_probs = target_probs[prob_indicies, draft_token_ids] # Shape: (speculate_k,)

        # Modified Rejection Sampling
        # A method to recover the distribution of the target model from samples from the draft
        # model, and logits of said tokens from both models.
        accept_draft_prob = torch.minimum(torch.ones(()), selected_target_probs / selected_draft_probs)
        rejected_locations = (torch.rand_like(accept_draft_prob) > accept_draft_prob).nonzero()

        if rejected_locations.shape[0] == 0:
            # All draft tokens are accepted and we generate a bonus token from the 
            # last token's probability distribution from the `target_probs`
            bonus_token_id = SpeculativeDecoding.multinomial_sample_one_no_sync(target_probs[-1])
            # Fill the last token into the draft model
            self._draft_sampler.forward(input_ids=draft_token_ids[-1].view(1, -1), use_cache=True)
            return torch.cat((draft_token_ids, bonus_token_id)) # Shape: (speculate_k + 1,)
        else:
            # `rejected_locations` contains indexes. Select the first index and consider draft 
            # tokens upto that index. Ignore the rest of the tokens.
            accept_length = rejected_locations[0].item()
            p = draft_probs[accept_length]
            q = target_probs[accept_length]
            probs = torch.clamp(q - p, min=torch.finfo(torch.float32).tiny)
            probs = probs / probs.sum()
            next_token_id = GreedySampler.multinomial_sample_one_no_sync(probs)
            return torch.cat((draft_token_ids[:accept_length], next_token_id))

    def _device_sync(self) -> None:
        
        device = self.device.type
        if "cuda" in device:
            # Block the program execution until all the current CUDA 
            # operations are completed
            torch.cuda.synchronize(device)
        elif "cpu" in device:
            pass
        else:
            print(f"device={device} is not yet suppported")
    
    def _compile(self) -> None:
        
        # Similar to https://github.com/Lightning-AI/litgpt/blob/f241d94df59d82b2017bfdcd3800ac8779eb45f5/generate/base.py#L161
        torch._dynamo.config.automatic_dynamic_shapes = True
        torch._inductor.config.triton.unique_kernel_names = True
        torch._inductor.config.coordinate_descent_tuning = True

        # `mode="reduce-overhead"` - uses CUDA graphs to reduce the overhead of Python
        # `fullgraph=True` - no graph breaks to maximize speedup
        self.model = torch.compile(self.model, mode="reduce-overhead", fullgraph=True)
        self.draft_model = torch.compile(self.draft_model, mode="reduce-overhead", fullgraph=True)
        
        # Compiling `generate_one_token` method of `Sampler` causes unsupported user defined errors
        # To make the compilation work, `Sampler` must be refactored to consist only static methods, removal of pydantic classes etc.